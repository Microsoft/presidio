{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymize PII Entities in text files\n",
    "\n",
    "<br>Using Presidio, anonymize PII content of files in an Azure Storage account.\n",
    "\n",
    "<br>The following code sample will:\n",
    "<ol>\n",
    "<li>Import the content of text files located in an Azure Storage blob folder</li>\n",
    "<li>Anonymize the content using Presidio</li>\n",
    "<li>Write the anonymized content back to the Azure Storage blob account</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import  BlobServiceClient\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import AnonymizerConfig\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import input_file_name, regexp_replace\n",
    "\n",
    "dbutils.widgets.text(\"storage_account_name\", \"\", \"Blob Storage Account Name\")\n",
    "dbutils.widgets.text(\"storage_container_name\", \"\", \"Blob Container Name\")\n",
    "dbutils.widgets.text(\"storage_account_access_key\", \"\", \"Storage Account Access Key\")\n",
    "dbutils.widgets.text(\"storage_input_folder\", \"input\", \"Input Folder\")\n",
    "dbutils.widgets.text(\"storage_output_folder\", \"output\", \"Output Folder\")\n"
   ]
  },
  {
   "source": [
    "# Import the text files from Azure Blob storage\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "storage_account_name = dbutils.widgets.get(\"storage_account_name\")\n",
    "storage_container_name = dbutils.widgets.get(\"storage_container_name\")\n",
    "storage_account_access_key = dbutils.widgets.get(\"storage_account_access_key\")\n",
    "\n",
    "\n",
    "blob_service_client = BlobServiceClient(account_url=\"https://\" + storage_account_name + \".blob.core.windows.net/\", credential=storage_account_access_key)\n",
    "container_client = blob_service_client.get_container_client(storage_container_name)\n",
    "\n",
    "blob_names = container_client.list_blobs(name_starts_with=dbutils.widgets.get(\"storage_input_folder\") + \"/\")\n",
    "blobs = [] \n",
    "for blob in blob_names:\n",
    "  blobs.append(\"wasbs://\" + storage_container_name + \"@\" + storage_account_name + \".blob.core.windows.net/\" + blob.name)\n",
    "  \n",
    "spark.conf.set(\"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\", storage_account_access_key)\n",
    "\n",
    "input_rdd = spark.read.text(blobs).withColumn(\"filename\", input_file_name())\n",
    "\n",
    "input_rdd.show()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Anonymize Text using Presidio\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_text(text):\n",
    "  analyzer = AnalyzerEngine()\n",
    "  anonymizer = AnonymizerEngine()\n",
    "  analyzer_results = analyzer.analyze(text=text, language='en')\n",
    "  anonymized_results = anonymizer.anonymize(\n",
    "    text=text,\n",
    "    analyzer_results=analyzer_results,    \n",
    "    anonymizers_config={\"DEFAULT\": AnonymizerConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"})}\n",
    "  )\n",
    "  return anonymized_results\n",
    "anonymized_rdd = input_rdd.rdd.map(lambda x: (x[\"value\"], x[\"filename\"], anonymize_text(x[\"value\"]))).toDF([\"text\", \"filename\", \"anonymized_text\"])\n",
    "\n",
    "anonymized_rdd.show()\n"
   ]
  },
  {
   "source": [
    "# Write the Anonymized content back to Azure Blob storage"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rdd = anonymized_rdd.withColumn('filename', regexp_replace('filename', \"^.*(/\" + dbutils.widgets.get(\"storage_input_folder\") + \"/)\", dbutils.widgets.get(\"storage_output_folder\") + \"/\"))\n",
    "\n",
    "def upload_to_blob(text, file_name):\n",
    "  blob_client = blob_service_client.get_blob_client(container=storage_container_name, blob=file_name)\n",
    "  blob_client.upload_blob(text)\n",
    "  return \"SAVED\"\n",
    "\n",
    "save_udf = udf(upload_to_blob, StringType())\n",
    "# Invoke UDF for each row of the Dataframe.\n",
    "out_df = output_rdd.withColumn(\"processed\", save_udf(output_rdd.anonymized_text, output_rdd.filename))\n",
    "\n",
    "# Check if all the rows are processed successfully.\n",
    "out_df.show() \n",
    "\n",
    "out_df.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}